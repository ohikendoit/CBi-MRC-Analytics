# -*- coding: utf-8 -*-
"""Job Description NER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bM6boxUR1lRKCQAjKPCkjYhAuCfOAFKc

**EntityRuler for identifying UNDP units - NER**
- Annotations with spaCy model and entityRuler using the web crawled datasets on UNDP Job Postings
"""

from google.colab import drive
drive.mount('/content/gdrive')

#Purpose of this script is to create rules-based traing dataset
#with particular focus to UNDP unit organization

import spacy
from spacy.lang.en import English
from spacy.pipeline import EntityRuler
import json
import random

def load_data(file):
    with open(file, "r", encoding="utf-8") as f:
        data = json.load(f)
    return (data)

def save_data(file, data):
    with open(file, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=3)

def create_training_data(file, type):
    data = load_data(file)
    patterns = []
    for item in data:
        pattern = {
                    "label": type,
                    "pattern": item
                  }
        patterns.append(pattern)
    return(patterns)

def generate_rules(patterns):
    nlp = English()
    ruler = EntityRuler(nlp)
    ruler.add_patterns(patterns)
    nlp.add_pipe(ruler)
    nlp.to_disk("/content/gdrive/MyDrive/UNDP-Organizational-UNIT-NER/unit_ner")

def test_model(model, text):
    doc = nlp(text)
    results = []
    entities = []
    for ent in doc.ents:
        entities.append((ent.start_char, ent.end_char, ent.label_))
    if len(entities)>0:
        results = [text, {"entities": entities}]
        print(results)
    return (results)

patterns = create_training_data("/content/gdrive/MyDrive/UNDP-Organizational-UNIT-NER/UNDP_units_original.json", "UNIT")
generate_rules(patterns)

nlp = spacy.load("/content/gdrive/MyDrive/UNDP-Organizational-UNIT-NER/unit_ner")
nlp.max_length = 1073740824

TRAIN_DATA = []
with open("/content/gdrive/MyDrive/UNDP-Organizational-UNIT-NER/undp_jobs.txt", "r", errors='ignore') as f:
    text = f.read()

    chunks = text.split("\t")
    for chunk in chunks:
        hits=[]
        results = test_model(nlp,chunk)
        if results != None:
            TRAIN_DATA.append(results)

print(len(TRAIN_DATA))
save_data("/content/gdrive/MyDrive/UNDP-Organizational-UNIT-NER/units_training_data.json", TRAIN_DATA)